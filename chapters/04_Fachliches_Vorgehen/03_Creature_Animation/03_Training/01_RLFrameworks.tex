\subsection{Auswahl des RL Frameworks} \label{RLFrameworks}
In den Abschnitten \ref{mlAgentsFramework} und \ref{neroRLFramework} wurden bereits zwei RL Frameworks vorgestellt, die beide zum Trainieren der generierten Kreaturen in Betracht gezogen wurden. Beide Frameworks haben Vor- und Nachteile im Bezug auf diese Aufgabe, was eine Entscheidung erschwert hat.\\

\noindent Ml-Agents unterstützt bereits von sich aus kontinuierliche Aktionsräume, welche bei der Bewegung der Kreaturen benötigt werden. Außerdem erlaubt es mehrere Agenten in einer Umgebung zu platzieren, um das Trainieren zu beschleunigen. Die vorherigen Tests mit der Walker Testumgebung in Unity \cite{walkerEnv} haben gezeigt, dass selbst eine zum Laufen optimierte zweibeinige Kreatur recht lange braucht, bis zuverlässig laufen kann. Bei einer prozedural generierten Kreatur ist zu erwarten, dass es noch länger dauert und darum sollten alle Möglichkeiten das Training zu beschleunigen verwendet werden. Das Problem mit ml-Agents liegt in der Auswertung der Ergebnisse. Im Rahmen dieser Ausarbeitung ist es auch notwendig sich kritisch mit den Ergebnissen der Projektgruppe auseinander zu setzten und für den Teil der CreatureAnimation sind vor allem die Statistiken des Trainings relevant. Ml-Agents erhebt zwar im Rahmen des Trainings Statistiken und speichert diese in einem Tensorboard ab, allerdings werden nur wenige Statistiken erhoben. Zusätzlich speichert das Framework nicht direkt die erhobenen Werte ab, sondern in jedem Schritt nur den Durchschnitt der Werte. Für eine Analyse würde dies bedeuten, dass die Ursprünglichen Werte bereits die Durchschnitte sind und daher am Ende der Durchschnitt des Durchschnittes betrachtet wird, was keine guten Ergebnisse liefert. \\

\noindent Die Vorteile von neroRL sind zum einen, dass es mehr Informationen in das Tensorboard schreibt und die direkten Werte dort einträgt, ohne diese vorher zu verarbeiten. Außerdem verlangt neroRL nur den Abstand zwischen Checkpoints und legt dann so viele Checkpoints an, wie nötigt, ohne alte zu überschreiben, wenn die maximale Anzahl erreicht ist. Es liefert die Möglichkeit das Training zu beschleunigen, indem mehrere Umgebungen mit jeweils einem Worker parallel ausgeführt und trainiert werden. Die Nachteile von neroRL sind, dass nur Diskrete und Multidiskrete Aktionsräume unterstütze werden, welche zum Animieren der Kreaturen voraussichtlich nicht ausreichen werden. Außerdem erlaubt es nicht mehrere Agenten in einer Umgebung zu platzieren, was besser skaliert als die Variante mit mehreren Umgebungen.\\

\noindent Es ist also eindeutig, dass, unabhängig davon welches Framework gewählt wird, das ausgewählte Framework noch angepasst werden muss, bevor es verwendet werden kann. Im Hinblick darauf wurde sich für das neroRL Framework entschieden. Dieses Framework auf kontinuierliche Aktionen und Umgebungen mit mehren Agenten zu erweitern erschien einfacher, als ml-Agents auf parallele Umgebungen zu erweitern und das Anlegen der Statistiken so zu ändern, dass sie für die notwendige Analyse geeignet sind. Ein weiterer Einfluss auf diese Entscheidung war, dass Marco Pleines, der Entwickler von neroRL als Betreuer in der Projektgruppe mitwirkt. Bei Fragen oder Problemen während der Anpassung können also deutlich schneller Feedback und Hilfestellungen gegeben werden, als es bei ml-Agents der Fall wäre.

\subsection{Anpassung von neroRL} \label{neroAnpassung}
Bevor neroRL zum Trainieren verwendet werden kann müssen zunächst die in Kapitel \ref{RLFrameworks} genannten Probleme beseitigt werden.\\

\noindent  Zuerst muss die Implementierung so erweitert werden, dass sie auch Umgebungen mit kontinuierlichen Aktionsräumen unterstützt. Dazu muss neben dem Samplingprozess auch die Klasse zum Starten des Trainings angepasst werden, damit diese beim Überprüfen der Umgebung auch für kontinuierliche Aktionsräume korrekt die Form erkennt. Um die Implementierung so einfach und übersichtlich wie möglich zu halten, wurde entschieden in diesem Schritt die Optionen für Diskrete und Multi diskrete Aktionsräume komplett aus der Implementierung zu entfernen. Da im Rahmen dieser Projektgruppe nur kontinuierliche Aktionsräume benötigt werden, genügt auch ein Framework, welches nur diese unterstützt. \\

\noindent In dem nächsten Schritt muss die Implementierung so erweitert werden, dass mehrere Agenten mit dem selben Verhalten in einer Umgebung vorhanden sein können und sie alle mit in das Training einbezogen werden. Hier wurde entschieden das vorhandene Verhalten mit mehreren parallelen Umgebungen zu erweitert, anstatt es zu ersetzten. Die resultierende Implementierung sollte also in der Lage sein das Training noch weiter zu beschleunigen indem nicht nur die Anzahl der Agenten in jeder Umgebung erhöht werden kann, sondern auch die Anzahl an Umgebungen die parallel zum Trainieren verwendet werden. Diese sollte mehr als ausreichend sein, um den zeitlichen Overhead durch das verarbeiten mehrerer Agenten pro Umgebung auszugleichen.\\

\noindent In einem letzten Schritt muss noch die Implementierung und Optimierung des verwendeten PPO Algorithmus betrachtet werden. Die aktuelle Implementierung ist für Diskrete Aktionen bestimmt und das Netzwerk kann daher nur aus einer stark begrenzten Anzahl an Aktionen auswählen. Bei einem kontinuierlichen Raum der für die Aktionen nur eine Ober- und Untergrenze festlegt gibt es deutlich mehr mögliche Aktionen. Daher kann es sein, dass die aktuelle Implementierung von PPO nicht genügt, um nach dem Training zufriedenstellende Ergebnisse zu liefern. In diesem Fall müssten zusätzliche Optimierungsverfahren, wie z. B. Squashing oder Normalisierung, recherchiert und in den Algorithmus integriert werden.